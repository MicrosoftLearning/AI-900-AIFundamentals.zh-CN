{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 物体检测\r\n",
        "\r\n",
        "*物体检测*是计算机视觉的一种形式，在这种形式中，机器学习模型被训练为对图像中的各个物体实例进行分类，并指示一个标记其位置的*边界框*。可以将其视为从*图像分类*（在此阶段模型回答问题“这是什么物体的图像”）到构建解决方案（在此阶段我们可以问模型“这个图像中有什么物体，它们在什么位置？”）的过程。\r\n",
        "\r\n",
        "![正在识别水果的机器人](./images/object-detection.jpg)\r\n",
        "\r\n",
        "例如，一家商店也许可以使用物体检测模型来实现自动结帐系统，该系统使用摄像头扫描传送带并能识别特定商品，而无需把每个商品都放在传送带上单独扫描。\r\n",
        "\r\n",
        "Microsoft Azure 中的**自定义视觉**认知服务为创建和发布自定义物体检测模型提供了基于云的解决方案。\r\n",
        "\r\n",
        "## 创建自定义视觉资源\r\n",
        "\r\n",
        "要使用自定义视觉服务，需要具有可用于训练模型的 Azure 资源，以及可用于发布模型以供应用程序使用的资源。在完成这些任务时，可以使用相同的资源，也可以为每项任务使用不同的资源以单独分配成本（如果两个资源在同一区域中创建）。用于其中一个（或两个）任务的资源可以是常规的**认知服务**资源，也可以是特定的**自定义视觉**资源。请按照以下说明创建一个新的**自定义视觉**资源（你也可以使用现有的资源）。\r\n",
        "\r\n",
        "1. 在新的浏览器标签页中打开 Azure 门户 ([https://portal.azure.com](https://portal.azure.com))，使用与你的 Azure 订阅关联的 Microsoft 帐户进行登录。\r\n",
        "2. 选择“**&#65291;创建资源**”按钮，搜索“*自定义视觉*”并以如下设置创建**自定义视觉**资源：\r\n",
        "    - **创建选项**：均可\r\n",
        "    - **订阅**： *你的 Azure 订阅*\r\n",
        "    - **资源组**： *选择或创建具有唯一名称的资源组*\r\n",
        "    - **名称**： *输入一个唯一名称*\r\n",
        "    - **训练位置**： *选择任何可用区域*\r\n",
        "    - **训练定价层**：中的机器人 F0\r\n",
        "    - **预测位置**： *与训练位置保持一致*\r\n",
        "    - **预测定价层**：中的机器人 F0\r\n",
        "\r\n",
        "    > **备注**：如果在你的订阅中已有 F0 自定义视觉服务，此处请选择“**S0**”。\r\n",
        "\r\n",
        "3. 等待资源创建完成。\r\n",
        "\r\n",
        "## 创建自定义视觉项目\r\n",
        "\r\n",
        "要训练物体检测模型，需要根据训练资源创建自定义视觉项目。为此，需要使用自定义视觉门户。\r\n",
        "\r\n",
        "1. 在新的浏览器选项卡中打开自定义视觉门户 ([https://customvision.ai](https://customvision.ai))，使用与你的 Azure 订阅关联的 Microsoft 帐户进行登录。\r\n",
        "2. 新建一个项目，设置如下：\r\n",
        "    - **名称**：商品检测\r\n",
        "    - **说明**：针对商品的物体检测。\r\n",
        "    - **资源**： *你之前创建的自定义视觉资源*\r\n",
        "    - **项目类型**：物体检测\r\n",
        "    - **领域**：常规\r\n",
        "3. 等待项目创建完毕并在浏览器中打开。\r\n",
        "\r\n",
        "## 添加图像并进行标记\r\n",
        "\r\n",
        "要训练物体检测模型，需要上传包含你希望模型识别的类的图像，并对这些图像进行标记以指示每个物体实例的边界框。\r\n",
        "\r\n",
        "1.从 https://aka.ms/fruit-objects 下载并提取训练图像。提取的文件夹包含一个水果的图像集合。**备注**：如果你无法访问训练图像，临时的应变方法是转到 https://www.github.com，然后转到 https://aka.ms/fruit-objects。 \r\n",
        "2. 在自定义视觉门户 [https://customvision.ai](https://customvision.ai) 中，确保你正在处理物体检测项目 _Grocery Detection_。然后选择“**添加图像**”，并上传提取文件夹中的所有图像。\r\n",
        "\r\n",
        "![通过单击“添加图像”上传下载的图像。](./images/fruit-upload.jpg)\r\n",
        "\r\n",
        "3. 上传图像后，选择第一个图像将其打开。\r\n",
        "4. 将鼠标悬停在图像中的任何物体上，直到显示一个自动检测到的区域，如下图所示。然后选择物体，并根据需要调整该区域大小，使其包围所选物体。\r\n",
        "\r\n",
        "![物体的默认区域](./images/object-region.jpg)\r\n",
        "\r\n",
        "也可以简单地围绕该物体进行拖动，创建一个区域。\r\n",
        "\r\n",
        "5. 当该区域包围所选物体时，添加一个具有适当物体类型的新标签（“*苹果*”、“*香蕉*”或“*橙子*”），如下所示：\r\n",
        "\r\n",
        "![图像中带有标签的物体](./images/object-tag.jpg)\r\n",
        "\r\n",
        "6. 在图像中选择各个物体并为其添加标签，根据需要调整区域大小并添加新标签。\r\n",
        "\r\n",
        "![图像中两个带有标签的物体](./images/object-tags.jpg)\r\n",
        "\r\n",
        "7. 使用右侧的“**>**”链接转至下一个图像，并为图像中的物体添加标签。然后按照这样的步骤继续处理整个图像集合，为每个苹果、香蕉和橙子添加标签。\r\n",
        "\r\n",
        "8. 标记完最后一个图像后，关闭“**图像细节**”编辑器，并在“**训练图像**”页面上的“**标签**”下选择“**已标记**”以查看所有带有标签的图像：\r\n",
        "\r\n",
        "![项目中带有标签的图像](./images/tagged-images.jpg)\r\n",
        "\r\n",
        "## 训练和测试模型\r\n",
        "\r\n",
        "你已为项目中的图像添加标签，现在可以训练模型了。\r\n",
        "\r\n",
        "1. 在自定义视觉项目中，单击“**训练**”以使用带有标签的图像训练物体检测模型。选择“**快速训练**”选项。\r\n",
        "2. 等待训练完成（可能需要 10 分钟左右），然后检查*精度*、*召回*率和 *mAP* 性能指标 - 这些指标用于衡量分类模型的预测准确度，且应该都很高。\r\n",
        "3. 单击页面右上角的“**快速测试**”，然后在“**图像 URL**”框中输入 `https://aka.ms/apple-orange` 并查看生成的预测结果。然后关闭“**快速测试**”窗口。\r\n",
        "\r\n",
        "## 发布并使用物体检测模型\r\n",
        "\r\n",
        "现在即可发布已训练的模型并在客户端应用程序中使用它。\r\n",
        "\r\n",
        "1. 单击“**性能**”页面左上角的“**&#128504; 发布**”来发布已训练的模型，设置如下：\r\n",
        "    - **模型名称**：detect-produce\r\n",
        "    - **预测资源**：*你的自定义视觉**预测**资源*。\r\n",
        "\r\n",
        "### (!)签入 \r\n",
        "是否使用了相同的模型名称“**detect-produce**”？ \r\n",
        "\r\n",
        "2. 发布后，单击“**性能**”页面右上角的“*设置*”(&#9881;) 图标以查看项目设置。然后在左侧的“**常规**”下复制**项目 ID**。向下滚动并将其粘贴到步骤 5 下的代码单元格中，替换“**YOUR_PROJECT_ID**”。 \r\n",
        "\r\n",
        "> （*如果在本练习开始时你没有创建**自定义视觉**资源，而是使用**认知服务**资源，可以在项目设置的右侧复制其密钥和终结点，并将其粘贴至下方的代码单元格中，然后运行它以查看结果。否则请继续完成以下步骤，获取自定义视觉预测资源的密钥和终结点。*）\r\n",
        "\r\n",
        "3. 单击“**项目设置**”页面左上角的“*项目库*”(&#128065;) 图标以返回到自定义视觉门户主页，此处现在会列出你的项目。\r\n",
        "\r\n",
        "4. 单击自定义视觉门户主页右上角的“*设置*”(&#9881;) 图标，查看自定义视觉服务的设置。然后在“**资源**”下展开预测资源（不是训练资源），并将资源的**密钥**和**终结点**值复制到步骤 5 下面的代码单元格中，分别替换“**YOUR_KEY**”和“**YOUR_ENDPOINT**”。\r\n",
        "\r\n",
        "### (!)签入 \r\n",
        "如果你使用的是**自定义视觉**资源，那你是否使用过**预测**资源（不是训练资源）？\r\n",
        "\r\n",
        "5. 通过单击“运行单元格”<span>&#9655;</span> 按钮（位于单元格的左侧）运行下面的代码单元格，将变量设置为你自己的项目 ID、密钥和终结点值。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "project_id = 'YOUR_PROJECT_ID' # Replace with your project ID\r\n",
        "cv_key = 'YOUR_KEY' # Replace with your prediction resource primary key\r\n",
        "cv_endpoint = 'YOUR_ENDPOINT' # Replace with your prediction resource endpoint\r\n",
        "\r\n",
        "model_name = 'detect-produce' # this must match the model name you set when publishing your model iteration exactly (including case)!\r\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692485387
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在你可以使用密钥和终结点通过自定义视觉客户端连接到自定义视觉物体检测模型。\r\n",
        "\r\n",
        "运行以下代码单元格，该代码单元格使用你的模型来检测图像中的各个商品。\r\n",
        "\r\n",
        "> **备注**：无需太担心代码的详细信息。它使用适用于自定义视觉服务的 Python SDK 向模型提交图像并检索检测到的物体的预测结果。每条预测结果都由类名（“*苹果*”、“*香蕉*”或“*橙子*”）和指示在图像中检测到预测物体的位置的*边界框*坐标组成。然后该代码会使用这些信息在图像上的每个物体周围绘制一个标记框。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\r\n",
        "from msrest.authentication import ApiKeyCredentials\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from PIL import Image, ImageDraw, ImageFont\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Load a test image and get its dimensions\r\n",
        "test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\r\n",
        "test_img = Image.open(test_img_file)\r\n",
        "test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\r\n",
        "\r\n",
        "# Get a prediction client for the object detection model\r\n",
        "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\r\n",
        "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\r\n",
        "\r\n",
        "print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\r\n",
        "\r\n",
        "# Detect objects in the test image\r\n",
        "with open(test_img_file, mode=\"rb\") as test_data:\r\n",
        "    results = predictor.detect_image(project_id, model_name, test_data)\r\n",
        "\r\n",
        "# Create a figure to display the results\r\n",
        "fig = plt.figure(figsize=(8, 8))\r\n",
        "plt.axis('off')\r\n",
        "\r\n",
        "# Display the image with boxes around each detected object\r\n",
        "draw = ImageDraw.Draw(test_img)\r\n",
        "lineWidth = int(np.array(test_img).shape[1]/100)\r\n",
        "object_colors = {\r\n",
        "    \"apple\": \"lightgreen\",\r\n",
        "    \"banana\": \"yellow\",\r\n",
        "    \"orange\": \"orange\"\r\n",
        "}\r\n",
        "for prediction in results.predictions:\r\n",
        "    color = 'white' # default for 'other' object tags\r\n",
        "    if (prediction.probability*100) > 50:\r\n",
        "        if prediction.tag_name in object_colors:\r\n",
        "            color = object_colors[prediction.tag_name]\r\n",
        "        left = prediction.bounding_box.left * test_img_w \r\n",
        "        top = prediction.bounding_box.top * test_img_h \r\n",
        "        height = prediction.bounding_box.height * test_img_h\r\n",
        "        width =  prediction.bounding_box.width * test_img_w\r\n",
        "        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\r\n",
        "        draw.line(points, fill=color, width=lineWidth)\r\n",
        "        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\r\n",
        "plt.imshow(test_img)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692585672
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "查看生成的预测结果，其中显示检测到的物体以及每种预测结果对应的概率。"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}